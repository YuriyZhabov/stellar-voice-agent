#!/usr/bin/env python3
"""
–°–∏—Å—Ç–µ–º–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞ LiveKit.
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –ø—Ä–æ–µ–∫—Ç–µ –∏ —Å–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é.
"""

import os
import ast
import json
import yaml
import time
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Set
from dataclasses import dataclass, asdict
import fnmatch

@dataclass
class FileInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ –≤ –ø—Ä–æ–µ–∫—Ç–µ."""
    path: str
    name: str
    extension: str
    size: int
    category: str
    subcategory: str
    status: str  # 'ready', 'incomplete', 'error', 'missing'
    syntax_valid: bool
    dependencies: List[str]
    issues: List[str]
    quality_score: float
    last_modified: str
    content_preview: str

class FileScanner:
    """–°–∫–∞–Ω–∏—Ä—É–µ—Ç —Ñ–∞–π–ª–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –ø—Ä–æ–µ–∫—Ç–∞."""
    
    def __init__(self, root_path: str):
        self.root_path = Path(root_path)
        self.exclude_patterns = [
            '.git', '.kiro', '__pycache__', '*.pyc', '.pytest_cache',
            'node_modules', '.venv', 'venv', '*.egg-info',
            '.DS_Store', 'Thumbs.db', '*.log'
        ]
        self.logger = logging.getLogger(__name__)
    
    def should_exclude(self, path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –¥–æ–ª–∂–µ–Ω –ª–∏ —Ñ–∞–π–ª –±—ã—Ç—å –∏—Å–∫–ª—é—á–µ–Ω."""
        path_str = str(path)
        
        for pattern in self.exclude_patterns:
            if fnmatch.fnmatch(path.name, pattern):
                return True
            if fnmatch.fnmatch(path_str, f"*/{pattern}/*"):
                return True
        
        return False
    
    def scan_project(self) -> List[Path]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –ø—Ä–æ–µ–∫—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤."""
        files = []
        
        for root, dirs, filenames in os.walk(self.root_path):
            root_path = Path(root)
            
            # –ò—Å–∫–ª—é—á–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            dirs[:] = [d for d in dirs if not self.should_exclude(root_path / d)]
            
            for filename in filenames:
                file_path = root_path / filename
                if not self.should_exclude(file_path):
                    files.append(file_path)
        
        return files
    
    def get_file_info(self, file_path: Path) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ."""
        try:
            stat = file_path.stat()
            
            # –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è –ø—Ä–µ–≤—å—é
            content_preview = ""
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content_preview = f.read(500)  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
            except Exception:
                try:
                    with open(file_path, 'r', encoding='latin-1') as f:
                        content_preview = f.read(500)
                except Exception:
                    content_preview = "[Binary file or encoding error]"
            
            return {
                'path': str(file_path.relative_to(self.root_path)),
                'name': file_path.name,
                'extension': file_path.suffix.lower(),
                'size': stat.st_size,
                'last_modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'content_preview': content_preview
            }
        except Exception as e:
            self.logger.error(f"Error getting info for {file_path}: {e}")
            return {
                'path': str(file_path.relative_to(self.root_path)),
                'name': file_path.name,
                'extension': file_path.suffix.lower(),
                'size': 0,
                'last_modified': datetime.now().isoformat(),
                'content_preview': f"[Error reading file: {e}]"
            }

class FileClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ñ–∞–π–ª—ã –ø–æ —Ç–∏–ø–∞–º –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º."""
    
    def __init__(self):
        self.categories = {
            'source_code': {
                'extensions': ['.py'],
                'subcategories': {
                    'api_clients': ['client', 'api'],
                    'auth': ['auth', 'token', 'jwt'],
                    'monitoring': ['monitor', 'health', 'alert', 'log'],
                    'security': ['security', 'encrypt', 'hash'],
                    'services': ['service', 'egress', 'ingress'],
                    'integration': ['integration', 'webhook'],
                    'config': ['config', 'setting'],
                    'main': ['main', 'app', 'server'],
                    'utils': ['util', 'helper', 'tool']
                }
            },
            'tests': {
                'extensions': ['.py'],
                'patterns': ['test_*.py', '*_test.py', 'conftest.py', 'pytest.ini'],
                'subcategories': {
                    'unit': ['test_'],
                    'integration': ['integration', 'flow'],
                    'load': ['load', 'performance', 'stress'],
                    'security': ['security'],
                    'validation': ['validation', 'verify']
                }
            },
            'config': {
                'extensions': ['.yaml', '.yml', '.json', '.env', '.ini', '.toml'],
                'subcategories': {
                    'deployment': ['deploy', 'docker', 'k8s'],
                    'monitoring': ['monitor', 'alert'],
                    'security': ['security', 'auth'],
                    'performance': ['performance', 'retry'],
                    'main': ['config', 'setting']
                }
            },
            'docs': {
                'extensions': ['.md', '.rst', '.txt'],
                'subcategories': {
                    'guides': ['guide', 'setup', 'install'],
                    'api_docs': ['api', 'endpoint'],
                    'examples': ['example', 'sample'],
                    'reports': ['report', 'summary'],
                    'architecture': ['architecture', 'design']
                }
            },
            'scripts': {
                'extensions': ['.py', '.sh', '.bash'],
                'patterns': ['scripts/*'],
                'subcategories': {
                    'deployment': ['deploy', 'migrate', 'rollback'],
                    'validation': ['validate', 'test', 'check'],
                    'utilities': ['util', 'tool', 'helper'],
                    'automation': ['auto', 'run']
                }
            }
        }
    
    def classify_file(self, file_info: Dict[str, Any]) -> tuple:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é."""
        path = file_info['path']
        name = file_info['name'].lower()
        extension = file_info['extension']
        content = file_info['content_preview'].lower()
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ç–µ—Å—Ç–æ–≤
        if self._is_test_file(path, name):
            subcategory = self._get_test_subcategory(name, content)
            return 'tests', subcategory
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Å–∫—Ä–∏–ø—Ç–æ–≤
        if path.startswith('scripts/'):
            subcategory = self._get_script_subcategory(name, content)
            return 'scripts', subcategory
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        for category, config in self.categories.items():
            if extension in config.get('extensions', []):
                if category == 'tests':  # –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤—ã—à–µ
                    continue
                
                subcategory = self._get_subcategory(category, name, content, path)
                return category, subcategory
        
        # –ù–µ–∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        return 'other', 'unknown'
    
    def _is_test_file(self, path: str, name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª —Ç–µ—Å—Ç–æ–º."""
        return (name.startswith('test_') or 
                name.endswith('_test.py') or 
                name == 'conftest.py' or
                'test' in path.split('/'))
    
    def _get_test_subcategory(self, name: str, content: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ç–µ—Å—Ç–∞."""
        if 'load' in name or 'performance' in name or 'stress' in name:
            return 'load'
        elif 'integration' in name or 'flow' in name:
            return 'integration'
        elif 'security' in name:
            return 'security'
        elif 'validation' in name or 'verify' in name:
            return 'validation'
        else:
            return 'unit'
    
    def _get_script_subcategory(self, name: str, content: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å–∫—Ä–∏–ø—Ç–∞."""
        if any(word in name for word in ['deploy', 'migrate', 'rollback']):
            return 'deployment'
        elif any(word in name for word in ['validate', 'test', 'check']):
            return 'validation'
        elif any(word in name for word in ['run', 'auto']):
            return 'automation'
        else:
            return 'utilities'
    
    def _get_subcategory(self, category: str, name: str, content: str, path: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–ª—è —Ñ–∞–π–ª–∞."""
        subcategories = self.categories[category].get('subcategories', {})
        
        for subcategory, keywords in subcategories.items():
            if any(keyword in name or keyword in path for keyword in keywords):
                return subcategory
        
        return 'main'

class SyntaxAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—Å —Ñ–∞–π–ª–æ–≤."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def analyze_python_file(self, file_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç Python —Ñ–∞–π–ª."""
        result = {
            'syntax_valid': False,
            'imports': [],
            'issues': [],
            'functions': 0,
            'classes': 0,
            'lines': 0
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –ü–æ–¥—Å—á–µ—Ç —Å—Ç—Ä–æ–∫
            result['lines'] = len(content.splitlines())
            
            # –ü–∞—Ä—Å–∏–Ω–≥ AST
            tree = ast.parse(content)
            result['syntax_valid'] = True
            
            # –ê–Ω–∞–ª–∏–∑ AST
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        result['imports'].append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        result['imports'].append(node.module)
                elif isinstance(node, ast.FunctionDef):
                    result['functions'] += 1
                elif isinstance(node, ast.ClassDef):
                    result['classes'] += 1
            
            # –ü–æ–∏—Å–∫ TODO/FIXME –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
            for line_num, line in enumerate(content.splitlines(), 1):
                line_lower = line.lower()
                if any(marker in line_lower for marker in ['todo', 'fixme', 'xxx', 'hack']):
                    result['issues'].append(f"Line {line_num}: {line.strip()}")
            
        except SyntaxError as e:
            result['issues'].append(f"Syntax error: {e}")
        except Exception as e:
            result['issues'].append(f"Analysis error: {e}")
        
        return result
    
    def analyze_yaml_file(self, file_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç YAML —Ñ–∞–π–ª."""
        result = {
            'syntax_valid': False,
            'issues': [],
            'keys': []
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            
            result['syntax_valid'] = True
            
            if isinstance(data, dict):
                result['keys'] = list(data.keys())
            
        except yaml.YAMLError as e:
            result['issues'].append(f"YAML error: {e}")
        except Exception as e:
            result['issues'].append(f"Analysis error: {e}")
        
        return result
    
    def analyze_json_file(self, file_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç JSON —Ñ–∞–π–ª."""
        result = {
            'syntax_valid': False,
            'issues': [],
            'keys': []
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            result['syntax_valid'] = True
            
            if isinstance(data, dict):
                result['keys'] = list(data.keys())
            
        except json.JSONDecodeError as e:
            result['issues'].append(f"JSON error: {e}")
        except Exception as e:
            result['issues'].append(f"Analysis error: {e}")
        
        return result

class QualityAssessor:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤."""
    
    def assess_file_quality(self, file_info: Dict[str, Any], analysis_result: Dict[str, Any]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–∞ (0-1)."""
        score = 1.0
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
        if not analysis_result.get('syntax_valid', True):
            score -= 0.5
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –ø—Ä–æ–±–ª–µ–º—ã
        issues_count = len(analysis_result.get('issues', []))
        score -= min(0.3, issues_count * 0.05)
        
        # –ë–æ–Ω—É—Å –∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
        if file_info['extension'] == '.py':
            lines = analysis_result.get('lines', 0)
            if lines > 0:
                # –ë–æ–Ω—É—Å –∑–∞ —Ä–∞–∑—É–º–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                if 50 <= lines <= 500:
                    score += 0.1
                elif lines > 1000:
                    score -= 0.1
        
        return max(0.0, min(1.0, score))

class ProjectClassifier:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞."""
    
    def __init__(self, root_path: str = "."):
        self.root_path = Path(root_path)
        self.scanner = FileScanner(root_path)
        self.classifier = FileClassifier()
        self.syntax_analyzer = SyntaxAnalyzer()
        self.quality_assessor = QualityAssessor()
        self.logger = self._setup_logging()
    
    def _setup_logging(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(__name__)
    
    def classify_project(self) -> Dict[str, Any]:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –≤–µ—Å—å –ø—Ä–æ–µ–∫—Ç."""
        self.logger.info("–ù–∞—á–∞–ª–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞...")
        start_time = time.time()
        
        # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
        files = self.scanner.scan_project()
        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤")
        
        classified_files = []
        categories_stats = {}
        total_issues = 0
        
        for file_path in files:
            try:
                # –ü–æ–ª—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
                file_info = self.scanner.get_file_info(file_path)
                
                # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
                category, subcategory = self.classifier.classify_file(file_info)
                
                # –ê–Ω–∞–ª–∏–∑ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞
                analysis_result = self._analyze_file_syntax(file_path)
                
                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                quality_score = self.quality_assessor.assess_file_quality(file_info, analysis_result)
                
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
                status = self._determine_file_status(analysis_result, quality_score)
                
                # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ FileInfo
                classified_file = FileInfo(
                    path=file_info['path'],
                    name=file_info['name'],
                    extension=file_info['extension'],
                    size=file_info['size'],
                    category=category,
                    subcategory=subcategory,
                    status=status,
                    syntax_valid=analysis_result.get('syntax_valid', True),
                    dependencies=analysis_result.get('imports', []),
                    issues=analysis_result.get('issues', []),
                    quality_score=quality_score,
                    last_modified=file_info['last_modified'],
                    content_preview=file_info['content_preview'][:200]
                )
                
                classified_files.append(classified_file)
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                if category not in categories_stats:
                    categories_stats[category] = {}
                if subcategory not in categories_stats[category]:
                    categories_stats[category][subcategory] = 0
                categories_stats[category][subcategory] += 1
                
                total_issues += len(classified_file.issues)
                
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
        
        duration = time.time() - start_time
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        report = self._generate_report(classified_files, categories_stats, total_issues, duration)
        
        self.logger.info(f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {duration:.2f} —Å–µ–∫—É–Ω–¥")
        return report
    
    def _analyze_file_syntax(self, file_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—Å —Ñ–∞–π–ª–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞."""
        extension = file_path.suffix.lower()
        
        if extension == '.py':
            return self.syntax_analyzer.analyze_python_file(file_path)
        elif extension in ['.yaml', '.yml']:
            return self.syntax_analyzer.analyze_yaml_file(file_path)
        elif extension == '.json':
            return self.syntax_analyzer.analyze_json_file(file_path)
        else:
            return {'syntax_valid': True, 'issues': [], 'imports': []}
    
    def _determine_file_status(self, analysis_result: Dict[str, Any], quality_score: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç–∞—Ç—É—Å —Ñ–∞–π–ª–∞."""
        if not analysis_result.get('syntax_valid', True):
            return 'error'
        elif quality_score < 0.5:
            return 'incomplete'
        elif len(analysis_result.get('issues', [])) > 0:
            return 'incomplete'
        else:
            return 'ready'
    
    def _generate_report(self, files: List[FileInfo], categories_stats: Dict, total_issues: int, duration: float) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç."""
        total_files = len(files)
        ready_files = sum(1 for f in files if f.status == 'ready')
        error_files = sum(1 for f in files if f.status == 'error')
        avg_quality = sum(f.quality_score for f in files) / total_files if total_files > 0 else 0
        
        return {
            "classification_summary": {
                "timestamp": datetime.now().isoformat(),
                "duration_seconds": round(duration, 2),
                "total_files": total_files,
                "ready_files": ready_files,
                "error_files": error_files,
                "incomplete_files": total_files - ready_files - error_files,
                "total_issues": total_issues,
                "average_quality_score": round(avg_quality, 3),
                "categories_distribution": categories_stats
            },
            "files": [asdict(f) for f in files],
            "recommendations": self._generate_recommendations(files, categories_stats)
        }
    
    def _generate_recommendations(self, files: List[FileInfo], categories_stats: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ–µ–∫—Ç–∞."""
        recommendations = []
        
        error_files = [f for f in files if f.status == 'error']
        if error_files:
            recommendations.append(f"–ò—Å–ø—Ä–∞–≤–∏—Ç—å —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –≤ {len(error_files)} —Ñ–∞–π–ª–∞—Ö")
        
        incomplete_files = [f for f in files if f.status == 'incomplete']
        if incomplete_files:
            recommendations.append(f"–î–æ—Ä–∞–±–æ—Ç–∞—Ç—å {len(incomplete_files)} –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
        
        files_with_todos = [f for f in files if any('todo' in issue.lower() for issue in f.issues)]
        if files_with_todos:
            recommendations.append(f"–û–±—Ä–∞–±–æ—Ç–∞—Ç—å TODO –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ {len(files_with_todos)} —Ñ–∞–π–ª–∞—Ö")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è —Ç–µ—Å—Ç–∞–º–∏
        source_files = sum(categories_stats.get('source_code', {}).values())
        test_files = sum(categories_stats.get('tests', {}).values())
        if source_files > 0 and test_files / source_files < 0.5:
            recommendations.append("–£–≤–µ–ª–∏—á–∏—Ç—å –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ—Å—Ç–∞–º–∏ - –º–∞–ª–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤")
        
        if not recommendations:
            recommendations.append("–ü—Ä–æ–µ–∫—Ç –≤ —Ö–æ—Ä–æ—à–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏!")
        
        return recommendations

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    import argparse
    
    parser = argparse.ArgumentParser(description='–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞ LiveKit')
    parser.add_argument('--path', default='.', help='–ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: —Ç–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è)')
    parser.add_argument('--output', default='project_classification_report.json', help='–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞')
    
    args = parser.parse_args()
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞
    classifier = ProjectClassifier(args.path)
    report = classifier.classify_project()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # –í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    summary = report['classification_summary']
    print("\n" + "="*60)
    print("üìä –û–¢–ß–ï–¢ –ü–û –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –ü–†–û–ï–ö–¢–ê")
    print("="*60)
    print(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {summary['total_files']}")
    print(f"–ì–æ—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {summary['ready_files']}")
    print(f"–§–∞–π–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏: {summary['error_files']}")
    print(f"–ù–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {summary['incomplete_files']}")
    print(f"–í—Å–µ–≥–æ –ø—Ä–æ–±–ª–µ–º: {summary['total_issues']}")
    print(f"–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {summary['average_quality_score']}")
    print(f"–í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {summary['duration_seconds']} —Å–µ–∫")
    
    print("\nüìã –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
    for category, subcategories in summary['categories_distribution'].items():
        total_in_category = sum(subcategories.values())
        print(f"  {category}: {total_in_category}")
        for subcategory, count in subcategories.items():
            print(f"    - {subcategory}: {count}")
    
    print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    for i, rec in enumerate(report['recommendations'], 1):
        print(f"  {i}. {rec}")
    
    print(f"\nüìÑ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {args.output}")

if __name__ == "__main__":
    main()